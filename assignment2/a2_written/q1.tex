\titledquestion{Understanding word2vec}[15]
Recall that the key insight behind {\tt word2vec} is that \textit{`a word is known by the company it keeps'}. Concretely, consider a `center' word $c$ surrounded before and after by a context of a certain length. We term words in this contextual window `outside words' ($O$). For example, in Figure~\ref{fig:word2vec}, the context window length is 2, the center word $c$ is `banking', and the outside words are `turning', `into', `crises', and `as':

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{word2vec.png}
    \caption{The word2vec skip-gram prediction model with window size 2}
    \label{fig:word2vec}
\end{figure}

Skip-gram {\tt word2vec} aims to learn the probability distribution $P(O|C)$. 
Specifically, given a specific word $o$ and a specific word $c$, we want to predict $P(O=o|C=c)$: the probability that word $o$ is an `outside' word for $c$ (i.e., that it falls within the contextual window of $c$).
We model this probability by taking the softmax function over a series of vector dot-products: % I added the word "softmax" here because I bet a lot of students will have forgotten what softmax is and why the loss fn is called naive softmax. but if this is too wordy we can just take it out

\begin{equation}
 P(O=o \mid C=c) = \frac{\exp(\bu_{o}^\top \bv_c)}{\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c)}
 \label{word2vec_condprob}
\end{equation}

For each word, we learn vectors $u$ and $v$, where $\bu_o$ is the `outside' vector representing outside word $o$, and $\bv_c$ is the `center' vector representing center word $c$. 
We store these parameters in two matrices, $\bU$ and $\bV$.
The columns of $\bU$ are all the `outside' vectors $\bu_{w}$;
the columns of $\bV$ are all of the `center' vectors $\bv_{w}$. 
Both $\bU$ and $\bV$ contain a vector for every $w \in \text{Vocabulary}$.\footnote{Assume that every word in our vocabulary is matched to an integer number $k$. Bolded lowercase letters represent vectors. $\bu_{k}$ is both the $k^{th}$ column of $\bU$ and the `outside' word vector for the word indexed by $k$. $\bv_k$ is both the $k^{th}$ column of $\bV$ and the `center' word vector for the word indexed by $k$. \textbf{In order to simplify notation we shall interchangeably use $k$ to refer to word $k$ and the index of word $k$.}}\newline

%We can think of the probability distribution $P(O|C)$ as a prediction function that we can approximate via supervised learning. For any training example, we will have a single $o$ and $c$. We will then compute a value $P(O=o|C=c)$ and report the loss. 
Recall from lectures that, for a single pair of words $c$ and $o$, the loss is given by:

\begin{equation} 
\bJ_{\text{naive-softmax}}(\bv_c, o, \bU) = -\log P(O=o| C=c).
\label{naive-softmax}
\end{equation}

We can view this loss as the cross-entropy\footnote{The \textbf{cross-entropy loss} between the true (discrete) probability distribution $p$ and another distribution $q$ is $-\sum_i p_i \log(q_i)$.} between the true distribution $\by$ and the predicted distribution $\hat{\by}$, for a particular center word c and a particular outside word o. 
Here, both $\by$ and $\hat{\by}$ are vectors with length equal to the number of words in the vocabulary.
Furthermore, the $k^{th}$ entry in these vectors indicates the conditional probability of the $k^{th}$ word being an `outside word' for the given $c$. 
The true empirical distribution $\by$ is a one-hot vector with a 1 for the true outside word $o$, and 0 everywhere else, for this particular example of center word c and outside word o.\footnote{Note that the true conditional probability distribution of context words for the entire training dataset would not be one-hot.}
The predicted distribution $\hat{\by}$ is the probability distribution $P(O|C=c)$ given by our model in equation (\ref{word2vec_condprob}). \newline

\textbf{Note:} Throughout this homework, when computing derivatives, please use the method reviewed during the lecture (i.e. no Taylor Series Approximations).

\clearpage
\begin{parts}
\part[2]
Prove that the naive-softmax loss (Equation \ref{naive-softmax}) is the same as the cross-entropy loss between $\by$  and $\hat{\by}$, i.e. (note that $\by$ 
 (true distribution), $\hat{\by}$ (predicted distribution) are vectors and $\hat{\by}_o$ is a scalar):

\begin{equation} 
-\sum_{w \in \text{Vocab}} \by_w \log(\hat{\by}_w) = - \log (\hat{\by}_o).
\end{equation}

Your answer should be one line. You may describe your answer in words.
\ifans{
\begin{equation} 
\by_w =  
\left\{
    \begin {aligned}
         & 1 \quad & w = o \\
         & 0 \quad & w \neq o                  
    \end{aligned}
\right.
\Rightarrow 
-\sum_{w \in \text{Vocab}} \by_w \log(\hat{\by}_w) = - \log (\hat{\by}_o).
\end{equation}
}
% Question 1-B
\part[6]
\begin{enumerate}[label=\roman*.]
    \item 
    Compute the partial derivative of $\bJ_{\text{naive-softmax}}(\bv_c, o, \bU)$ with respect to $\bv_c$. \emph{Please write your answer in terms of $\by$, $\hat{\by}$, $\bU$, and show your work to receive full credit}.
    \begin{itemize} 
        \item \textbf{Note}: Your final answers for the partial derivative should follow the shape convention: the partial derivative of any function $f(x)$ with respect to $x$ should have the \textbf{same shape} as $x$.\footnote{This allows us to efficiently minimize a function using gradient descent without worrying about reshaping or dimension mismatching. While following the shape convention, we're guaranteed that $\theta:= \theta - \alpha\frac{\partial J(\theta)}{\partial \theta}$ is a well-defined update rule.}
        \item Please provide your answers for the partial derivative in vectorized form. For example, when we ask you to write your answers in terms of $\by$, $\hat{\by}$, and $\bU$, you may not refer to specific elements of these terms in your final answer (such as $\by_1$, $\by_2$, $\dots$). 
    \end{itemize}
    \item
    When is the gradient you computed equal to zero? \\
    \textbf{Hint:} You may wish to review and use some introductory linear algebra concepts.
    \item
    The gradient you found is the difference between the two terms. Provide an interpretation of how each of these terms improves the word vector when this gradient is subtracted from the word vector $v_c$.

\end{enumerate}

\ifans{
\begin{enumerate}[label=\roman*.]
    \item 
    \begin{gather*} 
    \bJ_{\text{naive-softmax}}(\bv_c, o, \bU) = 
    - \log (\hat{\by}_o) = -\log P(O=o| C=c) = -\log ( \frac{\exp(\bu_{o}^\top \bv_c)}{\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c)}) \\
    \nabla_{\bv_c} \bJ_{\text{naive-softmax}}(\bv_c, o, \bU) = 
     \frac{\partial}{\bv_c} (-\log \frac{\exp(\bu_{o}^\top \bv_c)}{\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c)})  = \\
    = -\frac{1}{\hat{\by}_o}  (\frac{ \frac{\partial}{\bv_c} \exp(\bu_{o}^\top \bv_c)}{\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c)} - \frac{\exp(\bu_{o}^\top \bv_c) (\frac{\partial}{\bv_c}\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c))}{(\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c))^2}) = \\ 
    = -\frac{1}{\hat{\by}_o} (\bu_o   \hat{\by}_o - \hat{\by}_o \sum_{w \in \text{Vocab}} \bu_w P(O=w| C=c) = -\bu_o  + \sum_{w \in \text{Vocab}} \bu_w P(O=w| C=c) \\ \\
    \nabla_{\bv_c} \bJ_{\text{naive-softmax}}(\bv_c, o, \bU) = \bu_o  - \sum_{w \in \text{Vocab}} \bu_w P(O=w| C=c) = U^T(\hat{\by} - \by )
    \end{gather*}

    \item 
    The gradient is zero when the predicted output is equal to the expected outpute, i.e. when the calculated mean embedding of all the $\bu_w$ (weighted by their probabilty of ocurrence) is equal to the actual $\bu_o$ observed.

    \item 
    \begin{equation*}
        \bv_c \gets \bv_c - \alpha \bu_o + \alpha \sum_{w \in \text{Vocab}} \bu_w P(O=w| C=c)
    \end{equation*}
    This equation moves the embedding $\bv_c$ in the opposite direction of $\bu_o$ (the embedding of the observed word) and in the direction of the mean embedding, averaged by the probability of ocurrence.
\end{enumerate}
}

% Question 1-C
\part[1]
In many downstream applications using word embeddings, L2 normalized vectors (e.g. $\mathbf{u}/||\mathbf{u}||_2$ where $||\mathbf{u}||_2 = \sqrt{\sum_i u_i^2}$) are used instead of their raw forms (e.g. $\mathbf{u}$). Letâ€™s consider a hypothetical downstream task of binary classification of phrases as being positive or negative, where you decide the sign based on the sum of individual embeddings of the words. When would L2 normalization take away useful information for the downstream task? When would it not?

\ifans{
\begin{gather*}
\bu_x = \alpha \bu_y \Rightarrow L_2(\bu_x) = \frac{\bu_x}{||\bu_x||_2} , L_2(\bu_y)=\frac{\alpha \bu_x}{||\alpha\bu_x||_2} = \frac{\bu_x}{||\bu_x||_2} = L_2(\bu_x)
\end{gather*}
Suppose we have a sentence with both negative (e.g. bad) and positive words (e.g best). In this case best may be $\alpha \bu_x$ where $\bu_x$ could be the word good. In such sentence, summing the bad embedding with the best embedding may classify the sentence as positive. If we apply L2 normalization to the sentence, we will end up summing the embeddings of bad and good that may classify now the sentence as negative. Instead, if we consider the relative positions of the embeddings (not the magnitudes) to make the classification, the L2 normalization will not affect the results.
}

\textbf{Hint:} Consider the case where $\mathbf{u}_x = \alpha\mathbf{u}_y$ for some words $x \neq y$ and some scalar $\alpha$. When $\alpha$ is positive, what will be the value of normalized  $\mathbf{u}_x$ and normalized $\mathbf{u}_y$? How might $\mathbf{u}_x$ and $\mathbf{u}_y$ be related for such a normalization to affect or not affect the resulting classification?


% Question 1-D
\part[5] 
Compute the partial derivatives of $\bJ_{\text{naive-softmax}}(\bv_c, o, \bU)$ with respect to each of the `outside' word vectors, $\bu_w$'s. There will be two cases: when $w=o$, the true `outside' word vector, and $w \neq o$, for all other words. Please write your answer in terms of $\by$, $\hat{\by}$, and $\bv_c$. In this subpart, you may use specific elements within these terms as well (such as $\by_1$, $\by_2$, $\dots$). Note that $\bu_w$ is a vector while $\by_1, \by_2, \dots$ are scalars. Show your work to receive full credit.

\ifans{
\begin{gather*} 
w = o \Rightarrow \nabla_{\bu_o} \bJ_{\text{naive-softmax}}(\bv_c, o, \bU) = 
 \frac{\partial}{\bu_o} (-\log \frac{\exp(\bu_{o}^\top \bv_c)}{\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c)})  = \\
= -\frac{1}{\hat{\by}_o}  (\frac{ \frac{\partial}{\bu_o} \exp(\bu_{o}^\top \bv_c)}{\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c)} - \frac{\exp(\bu_{o}^\top \bv_c) (\frac{\partial}{\bu_o}\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c))}{(\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c))^2}) = \\ 
= -\frac{1}{\hat{\by}_o} (\bv_c \hat{\by}_o - \hat{\by}_o \bv_c \hat{\by}_o) = -\bv_c + \bv_c * \hat{\by}_o
\end{gather*}
\begin{gather*} 
w \neq o \Rightarrow \nabla_{\bu_w} \bJ_{\text{naive-softmax}}(\bv_c, o, \bU) = 
 \frac{\partial}{\bu_w} (-\log \frac{\exp(\bu_{o}^\top \bv_c)}{\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c)})  = \\
= -\frac{1}{\hat{\by}_o}  ( 0 - \frac{\exp(\bu_{o}^\top \bv_c) (\frac{\partial}{\bu_w}\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c))}{(\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c))^2}) = \\ 
= -\frac{1}{\hat{\by}_o} (0 - \hat{\by}_o \sum_{w \in \text{Vocab}} \bv_c P(O=w| C=c) =   \sum_{w \in \text{Vocab}} \bv_c P(O=w| C=c)
\end{gather*}
\begin{gather*} 
\nabla_{\bu_w} \bJ_{\text{naive-softmax}}(\bv_c, o, \bU) = \bv_c( \hat{\by} - \by)
\end{gather*}
}

% Question 1-E
\part[1]
Write down the partial derivative of $\bJ_{\text{naive-softmax}}(\bv_c, o, \bU)$ with respect to $\bU$. Please break down your answer in terms of the column vectors $\frac{\partial \bJ(\bv_c, o, \bU)}{\partial \bu_1}$, $\frac{\partial \bJ(\bv_c, o, \bU)}{\partial \bu_2}$, $\cdots$, $\frac{\partial \bJ(\bv_c, o, \bU)}{\partial \bu_{|\text{Vocab}|}}$. No derivations are necessary, just an answer in the form of a matrix.

\ifans{
\begin{equation*}
     \nabla_{U} \bJ_{\text{naive-softmax}}(\bv_c, o, \bU) = 
     \begin{bmatrix}
     \bv_c \hat{\by_1} \\
     \bv_c \hat{\by_2} \\
     \vdots \\
     \bv_c(\hat{\by_o}-1) \\
     \vdots \\
     \bv_c \hat{y_{|vocab|}} \\
     \end{bmatrix}
\end{equation*}
}

\end{parts}